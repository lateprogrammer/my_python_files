{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 1: pyspark introduction and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Age  Experience\n",
       "0    A   24           4\n",
       "1    B   26           6\n",
       "2    C   31          11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Book1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.read_csv('Book1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pyspark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.16:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x212677dfc10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('Book1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "| _c0|_c1|       _c2|\n",
      "+----+---+----------+\n",
      "|Name|Age|Experience|\n",
      "|   A| 24|         4|\n",
      "|   B| 26|         6|\n",
      "|   C| 31|        11|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show() # header not recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('Book1.csv')\n",
    "# Header issue fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 2: pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark df\n",
    "# read dataset\n",
    "# check datatypes of columns\n",
    "# columns select and indexing\n",
    "# describe\n",
    "# add columns\n",
    "# drop columns\n",
    "# Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the session if not done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.options('header', 'true').csv('Book1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the schema (datatypes)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default considers all variables as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('Book1.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|Name|Age|Experience|\n",
      "+----+---+----------+\n",
      "|   A| 24|         4|\n",
      "|   B| 26|         6|\n",
      "|   C| 31|        11|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the schema (datatypes)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way\n",
    "# df_pyspark = spark.read.csv('Book1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns # Like python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='A', Age=24, Experience=4),\n",
       " Row(Name='B', Age=26, Experience=6),\n",
       " Row(Name='C', Age=31, Experience=11)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3) # It is row wise not like pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|   A|\n",
      "|   B|\n",
      "|   C|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|Experience|\n",
      "+----+----------+\n",
      "|   A|         4|\n",
      "|   B|         6|\n",
      "|   C|        11|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name', 'Experience']).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark['Name'].show() #This doesnt work on pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes # llr to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+-----------------+\n",
      "|summary|Name|              Age|       Experience|\n",
      "+-------+----+-----------------+-----------------+\n",
      "|  count|   3|                3|                3|\n",
      "|   mean|null|             27.0|              7.0|\n",
      "| stddev|null|3.605551275463989|3.605551275463989|\n",
      "|    min|   A|               24|                4|\n",
      "|    max|   C|               31|               11|\n",
      "+-------+----+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()\n",
    "# It takes all the column types even strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------------------------+\n",
      "|Name|Age|Experience|Experience after 2 years|\n",
      "+----+---+----------+------------------------+\n",
      "|   A| 24|         4|                       6|\n",
      "|   B| 26|         6|                       8|\n",
      "|   C| 31|        11|                      13|\n",
      "+----+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Adding columns in df\n",
    "\n",
    "df_pyspark = df_pyspark.withColumn('Experience after 2 years', df_pyspark['Experience']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|Name|Age|Experience|\n",
      "+----+---+----------+\n",
      "|   A| 24|         4|\n",
      "|   B| 26|         6|\n",
      "|   C| 31|        11|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##  Drop the columns\n",
    "\n",
    "df_pyspark = df_pyspark.drop('Experience after 2 years') # can also take the list of columns\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|NewName|Age|Experience|\n",
      "+-------+---+----------+\n",
      "|      A| 24|         4|\n",
      "|      B| 26|         6|\n",
      "|      C| 31|        11|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Rename the columns\n",
    "\n",
    "df_pyspark=df_pyspark.withColumnRenamed('Name', 'NewName')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 3: Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "# Dropping rows\n",
    "# Various parameters in dropping functionalities\n",
    "# Handing missing values by mean, median, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "|   A|  24|         4| 10000|\n",
      "|   B|  26|         6| 45000|\n",
      "|   C|  31|        11| 36000|\n",
      "|   D|  41|         7| 42000|\n",
      "|   E|null|      null| 23000|\n",
      "+----+----+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv('book1.csv', inferSchema=True, header=True)\n",
    "sdf.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  24|         4| 10000|\n",
      "|  26|         6| 45000|\n",
      "|  31|        11| 36000|\n",
      "|  41|         7| 42000|\n",
      "|null|      null| 23000|\n",
      "|  34|        12| 36000|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop columns\n",
    "\n",
    "sdf.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop NaN values\n",
    "\n",
    "sdf.na.drop().show() # Whereever null values, all rows will get removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop has 3 params. how, thresh, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### how==any\n",
    "\n",
    "# sdf.na.drop(how='all').show() #only all the values are NULL it will get drop\n",
    "sdf.na.drop(how='any').show() #the above o[p] where one of null values, it gets dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "|   A|  24|         4| 10000|\n",
      "|   B|  26|         6| 45000|\n",
      "|   C|  31|        11| 36000|\n",
      "|   D|  41|         7| 42000|\n",
      "|   E|null|      null| 23000|\n",
      "|   F|  34|        12| 36000|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.na.drop(how='any', thresh=2).show() # Rows with atleast 2 non null values, it will preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.na.drop(how='any', thresh=3).show() # Rows with atleast 3 non null values, it will preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E got dropped since it doesnt have atleast 3 non null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.na.drop(how='any', subset='Experience').show() \n",
    "# Drops a record only when experience has a null value\n",
    "# can also mention multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   E| 99|        99| 23000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.fillna(99, ['Experience', 'Age']).show() \n",
    "#NA values imputed in 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "|   A|  24|         4| 10000|\n",
      "|   B|  26|         6| 45000|\n",
      "|   C|  31|        11| 36000|\n",
      "|   D|  41|         7| 42000|\n",
      "|   E|null|      null| 23000|\n",
      "|   F|  34|        12| 36000|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputer function\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "            inputCols=['Age', 'Experience'],\n",
    "            outputCols=['{}_imputed'.format(c) for c in ['Age', 'Experience']]\n",
    "            ).setStrategy(\"mean\") # can also do median or mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+-----------+------------------+\n",
      "|Name| Age|Experience|Salary|Age_imputed|Experience_imputed|\n",
      "+----+----+----------+------+-----------+------------------+\n",
      "|   A|  24|         4| 10000|         24|                 4|\n",
      "|   B|  26|         6| 45000|         26|                 6|\n",
      "|   C|  31|        11| 36000|         31|                11|\n",
      "|   D|  41|         7| 42000|         41|                 7|\n",
      "|   E|null|      null| 23000|         31|                 8|\n",
      "|   F|  34|        12| 36000|         34|                12|\n",
      "+----+----+----------+------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(sdf).transform(sdf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 4: Pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter operation\n",
    "# &, |, ==\n",
    "# ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = sdf.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salary of the people less than = 20k\n",
    "\n",
    "sdf1.filter(\"Salary<=36000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 24|\n",
      "|   C| 31|\n",
      "|   F| 34|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.filter(\"Salary<=36000\").select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.filter(sdf1['Salary']<=36000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   C| 31|        11| 36000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.filter((sdf1['Salary']<=36000) & \n",
    "            (sdf1['Salary'] >=15000)).show() # AND operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   A| 24|         4| 10000|\n",
      "|   B| 26|         6| 45000|\n",
      "|   C| 31|        11| 36000|\n",
      "|   D| 41|         7| 42000|\n",
      "|   F| 34|        12| 36000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.filter((sdf1['Salary']<=36000) | \n",
    "            (sdf1['Salary'] >=15000)).show() # OR operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|   B| 26|         6| 45000|\n",
      "|   D| 41|         7| 42000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.filter(~(sdf1['Salary']<=36000)).show() # inverse filter operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Group by and Agg functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified the book1.csv for this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+-----------+\n",
      "|Name|Age|Experience|Salary| Department|\n",
      "+----+---+----------+------+-----------+\n",
      "|   A| 24|         4| 10000|        IOS|\n",
      "|   B| 26|         6| 45000|DataScience|\n",
      "|   C| 31|        11| 36000|DataScience|\n",
      "|   D| 41|         7| 42000|        IOS|\n",
      "|   E| 34|        12| 36000|DataScience|\n",
      "+----+---+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the data once again\n",
    "\n",
    "sdf = spark.read.csv('book1.csv', inferSchema=True, header=True)\n",
    "sdf.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|sum(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOS|      52000|\n",
      "|DataScience|     117000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "\n",
    "sdf.groupBy('Department').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|sum(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOS|      52000|\n",
      "|DataScience|     117000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Department max salary\n",
    "\n",
    "sdf.groupBy('Department').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|avg(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOS|    26000.0|\n",
      "|DataScience|    39000.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Department mean salary\n",
    "\n",
    "sdf.groupBy('Department').mean('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| Department|count|\n",
      "+-----------+-----+\n",
      "|        IOS|    2|\n",
      "|DataScience|    3|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     169000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.agg({'Salary': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|max(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOS|      42000|\n",
      "|DataScience|      45000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('Department').max('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|min(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOS|      10000|\n",
      "|DataScience|      36000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('Department').min('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
