{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('testing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|\"A wonderful litt...| positive|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf = spark.read.csv(\"IMDB Dataset.txt\", sep=\"\\t\", inferSchema=True, header=True)\n",
    "sparkdf.show(2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 , 2\n"
     ]
    }
   ],
   "source": [
    "print(sparkdf.count(),',', len(sparkdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           sentiment|count|\n",
      "+--------------------+-----+\n",
      "|            positive|24995|\n",
      "|            negative|24995|\n",
      "|\"\"Rock Star,\"\" or...|    1|\n",
      "|There is little m...|    1|\n",
      "|This is a story o...|    1|\n",
      "|This movie was sh...|    1|\n",
      "|The film stars Gi...|    1|\n",
      "|I would want to k...|    1|\n",
      "|The film, like ma...|    1|\n",
      "|The screenplay fo...|    1|\n",
      "|\"\"One Night at Mc...|    1|\n",
      "|Wynorski's credit...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf.groupBy('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are some review sentences in the sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = sparkdf.filter((sparkdf['sentiment']=='positive') | (sparkdf['sentiment']=='negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "| positive|24995|\n",
      "| negative|24995|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.groupBy('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def labelling(target_tabel):\n",
    "#     if target_label == 'positive':\n",
    "#         return '1'\n",
    "#     elif target_label == 'negative':\n",
    "#         return '0'\n",
    "#     else:\n",
    "#         return '-99'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_udf = F.udf(labelling, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = filtered_df.withColumn('target_label', label_udf(filtered_df['sentiment']))\n",
    "# # Didnt work\n",
    "\n",
    "# # Need to look into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df.createOrReplaceTempView('filtered_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_filtered_df = spark.sql('select * from filtered_df')\n",
    "# new_filtered_df.select('target_label').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_udf = F.udf(lambda sentiment: '1' if sentiment == 'positive' else '0', T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df.withColumn('target_label', exp_udf(filtered_df['sentiment'])).show(2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sparknlp\n",
    "# import sparknlp\n",
    "# sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|              review|        clean_review|sentiment|\n",
      "+--------------------+--------------------+---------+\n",
      "|One of the other ...|one of the other ...| positive|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = sparkdf.select('review', F.lower(F.regexp_replace('review', \"[^a-zA-Z\\\\s]\", \"\")).alias('clean_review'), 'sentiment')\n",
    "df_clean.show(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='clean_review', outputCol='word_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+\n",
      "|              review|        clean_review|sentiment|         word_tokens|\n",
      "+--------------------+--------------------+---------+--------------------+\n",
      "|One of the other ...|one of the other ...| positive|[one, of, the, ot...|\n",
      "|\"A wonderful litt...|a wonderful littl...| positive|[a, wonderful, li...|\n",
      "+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df = tokenizer.transform(df_clean)\n",
    "tokenized_df.show(2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenized_df.filter((tokenized_df['sentiment']=='positive') | (tokenized_df['sentiment']=='negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "| positive|24995|\n",
      "| negative|24995|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.groupBy('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords removal\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "no_stopwords = StopWordsRemover(inputCol='word_tokens', outputCol='refined_word_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|              review|        clean_review|sentiment|         word_tokens| refined_word_tokens|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|One of the other ...|one of the other ...| positive|[one, of, the, ot...|[one, reviewers, ...|\n",
      "|\"A wonderful litt...|a wonderful littl...| positive|[a, wonderful, li...|[wonderful, littl...|\n",
      "|\"I thought this w...|i thought this wa...| positive|[i, thought, this...|[thought, wonderf...|\n",
      "|Basically there's...|basically theres ...| negative|[basically, there...|[basically, there...|\n",
      "|\"Petter Mattei's ...|petter matteis lo...| positive|[petter, matteis,...|[petter, matteis,...|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df = no_stopwords.transform(tokenized_df)\n",
    "refined_df.show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+-----------------------+\n",
      "|len_review|len_clean_review|len_word_tokens|len_refined_word_tokens|\n",
      "+----------+----------------+---------------+-----------------------+\n",
      "|      1761|            1682|            307|                    170|\n",
      "|      1002|             954|            162|                     90|\n",
      "+----------+----------------+---------------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df = refined_df.withColumn('len_review', F.length('review'))\n",
    "refined_df = refined_df.withColumn('len_clean_review', F.length('clean_review'))\n",
    "refined_df = refined_df.withColumn('len_word_tokens', F.size('word_tokens'))\n",
    "refined_df = refined_df.withColumn('len_refined_word_tokens', F.size('refined_word_tokens'))\n",
    "refined_df.select('len_review', 'len_clean_review', 'len_word_tokens', 'len_refined_word_tokens').show(2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- clean_review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- word_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_word_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- len_review: integer (nullable = true)\n",
      " |-- len_clean_review: integer (nullable = true)\n",
      " |-- len_word_tokens: integer (nullable = false)\n",
      " |-- len_refined_word_tokens: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|sentiment|max(len_review)|\n",
      "+---------+---------------+\n",
      "| positive|          13710|\n",
      "| negative|           9001|\n",
      "+---------+---------------+\n",
      "\n",
      "+---------+---------------------+\n",
      "|sentiment|max(len_clean_review)|\n",
      "+---------+---------------------+\n",
      "| positive|                13331|\n",
      "| negative|                 8509|\n",
      "+---------+---------------------+\n",
      "\n",
      "+---------+--------------------+\n",
      "|sentiment|max(len_word_tokens)|\n",
      "+---------+--------------------+\n",
      "| positive|                2469|\n",
      "| negative|                1522|\n",
      "+---------+--------------------+\n",
      "\n",
      "+---------+----------------------------+\n",
      "|sentiment|max(len_refined_word_tokens)|\n",
      "+---------+----------------------------+\n",
      "| positive|                        1439|\n",
      "| negative|                         834|\n",
      "+---------+----------------------------+\n",
      "\n",
      "+---------+------------------+\n",
      "|sentiment|   avg(len_review)|\n",
      "+---------+------------------+\n",
      "| positive|1327.7240648129625|\n",
      "| negative|1297.5245849169835|\n",
      "+---------+------------------+\n",
      "\n",
      "+---------+---------------------+\n",
      "|sentiment|avg(len_clean_review)|\n",
      "+---------+---------------------+\n",
      "| positive|   1269.2199639927985|\n",
      "| negative|    1236.850650130026|\n",
      "+---------+---------------------+\n",
      "\n",
      "+---------+--------------------+\n",
      "|sentiment|avg(len_word_tokens)|\n",
      "+---------+--------------------+\n",
      "| positive|  232.62324464892978|\n",
      "| negative|   229.3028605721144|\n",
      "+---------+--------------------+\n",
      "\n",
      "+---------+----------------------------+\n",
      "|sentiment|avg(len_refined_word_tokens)|\n",
      "+---------+----------------------------+\n",
      "| positive|           123.8874174834967|\n",
      "| negative|          121.35567113422684|\n",
      "+---------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.groupBy('sentiment').max('len_review').show()\n",
    "refined_df.groupBy('sentiment').max('len_clean_review').show()\n",
    "refined_df.groupBy('sentiment').max('len_word_tokens').show()\n",
    "refined_df.groupBy('sentiment').max('len_refined_word_tokens').show()\n",
    "\n",
    "refined_df.groupBy('sentiment').avg('len_review').show()\n",
    "refined_df.groupBy('sentiment').avg('len_clean_review').show()\n",
    "refined_df.groupBy('sentiment').avg('len_word_tokens').show()\n",
    "refined_df.groupBy('sentiment').avg('len_refined_word_tokens').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = refined_df.withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always positive reviews had higher words\n",
    "# But average remains similar in positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='refined_word_tokens', outputCol='cv_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = cv.fit(refined_df).transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review',\n",
       " 'clean_review',\n",
       " 'sentiment',\n",
       " 'word_tokens',\n",
       " 'refined_word_tokens',\n",
       " 'len_review',\n",
       " 'len_clean_review',\n",
       " 'len_word_tokens',\n",
       " 'len_refined_word_tokens',\n",
       " 'id',\n",
       " 'cv_features']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         cv_features|\n",
      "+--------------------+\n",
      "|(175912,[0,1,4,13...|\n",
      "|(175912,[0,4,9,10...|\n",
      "|(175912,[0,1,4,7,...|\n",
      "|(175912,[0,1,2,3,...|\n",
      "|(175912,[0,2,3,4,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_df.select('cv_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', '', 'movie', 'film', 'one', 'like', 'good', 'even', 'time', 'really', 'see', 'story', 'much', 'well', 'get', 'great', 'also', 'bad', 'people', 'first', 'dont', 'movies', 'made', 'make', 'films']\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit(refined_df).vocabulary[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_vec = HashingTF(inputCol='refined_word_tokens', outputCol='tf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_df = hashing_vec.transform(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| refined_word_tokens|         tf_features|\n",
      "+--------------------+--------------------+\n",
      "|[one, reviewers, ...|(262144,[2325,328...|\n",
      "|[wonderful, littl...|(262144,[521,2574...|\n",
      "|[thought, wonderf...|(262144,[1043,139...|\n",
      "|[basically, there...|(262144,[6512,853...|\n",
      "|[petter, matteis,...|(262144,[2751,392...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_df.select('refined_word_tokens', 'tf_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review',\n",
       " 'clean_review',\n",
       " 'sentiment',\n",
       " 'word_tokens',\n",
       " 'refined_word_tokens',\n",
       " 'len_review',\n",
       " 'len_clean_review',\n",
       " 'len_word_tokens',\n",
       " 'len_refined_word_tokens',\n",
       " 'id',\n",
       " 'cv_features',\n",
       " 'tf_features']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features', outputCol='tf_idf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- clean_review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- word_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_word_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- len_review: integer (nullable = true)\n",
      " |-- len_clean_review: integer (nullable = true)\n",
      " |-- len_word_tokens: integer (nullable = false)\n",
      " |-- len_refined_word_tokens: integer (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      " |-- cv_features: vector (nullable = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df.createOrReplaceTempView('tf_idf_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = spark.sql('''SELECT *, \n",
    "                        CASE WHEN sentiment = 'positive' THEN 1 \n",
    "                        WHEN sentiment = 'negative' THEN  0 \n",
    "                        ELSE -99 END as target_label\n",
    "                        FROM tf_idf_df\n",
    "                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- clean_review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- word_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_word_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- len_review: integer (nullable = true)\n",
      " |-- len_clean_review: integer (nullable = true)\n",
      " |-- len_word_tokens: integer (nullable = false)\n",
      " |-- len_refined_word_tokens: integer (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      " |-- cv_features: vector (nullable = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      " |-- target_label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text_df = final_df.select(['len_review', 'len_clean_review', 'len_word_tokens', 'len_refined_word_tokens'\n",
    "                                , 'cv_features', 'tf_features', 'tf_idf_features', 'target_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- len_review: integer (nullable = true)\n",
      " |-- len_clean_review: integer (nullable = true)\n",
      " |-- len_word_tokens: integer (nullable = false)\n",
      " |-- len_refined_word_tokens: integer (nullable = false)\n",
      " |-- cv_features: vector (nullable = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      " |-- target_label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_text_df.select(['len_review', 'tf_idf_features', 'target_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- len_review: integer (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      " |-- target_label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['len_review', 'tf_idf_features'], outputCol='features_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df_assembler.transform(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- len_review: integer (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      " |-- target_label: integer (nullable = false)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_df.randomSplit([0.7, 0.3], seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o375.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 55.0 failed 1 times, most recent failure: Lost task 1.0 in stage 55.0 (TID 183) (192.168.2.16 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.io.ObjectInputStream$HandleTable.markDependency(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readArray(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.io.ObjectInputStream$HandleTable.markDependency(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readArray(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-a15f94a653b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'target_label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o375.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 55.0 failed 1 times, most recent failure: Lost task 1.0 in stage 55.0 (TID 183) (192.168.2.16 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.io.ObjectInputStream$HandleTable.markDependency(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readArray(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.io.ObjectInputStream$HandleTable.markDependency(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.readArray(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n\tat java.io.ObjectInputStream.readSerialData(Unknown Source)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)\r\n\tat java.io.ObjectInputStream.readObject0(Unknown Source)\r\n\tat java.io.ObjectInputStream.defaultReadFields(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('target_label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupBy('target_label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "24ed7b47d72257dfbaec50c2d38528507604f10ac8a57ae0d3c57775cd6534dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
